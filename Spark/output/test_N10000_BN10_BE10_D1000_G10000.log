SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/spark-2.4.3-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hadoop-2.8.4/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Ivy Default Cache set to: /home/chris.arnault/.ivy2/cache
The jars for the packages stored in: /home/chris.arnault/.ivy2/jars
:: loading settings :: url = jar:file:/opt/spark-2.4.3-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
org.influxdb#influxdb-java added as a dependency
graphframes#graphframes added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-e9ca6de6-60ae-4243-a15c-0232349dccb3;1.0
	confs: [default]
	found org.influxdb#influxdb-java;2.14 in central
	found com.squareup.retrofit2#retrofit;2.4.0 in central
	found com.squareup.retrofit2#converter-moshi;2.4.0 in central
	found com.squareup.moshi#moshi;1.5.0 in central
	found com.squareup.okio#okio;1.13.0 in central
	found org.msgpack#msgpack-core;0.8.16 in central
	found com.squareup.okhttp3#okhttp;3.11.0 in central
	found com.squareup.okio#okio;1.14.0 in central
	found com.squareup.okhttp3#logging-interceptor;3.11.0 in central
	found graphframes#graphframes;0.7.0-spark2.4-s_2.11 in spark-packages
	found org.slf4j#slf4j-api;1.7.16 in central
:: resolution report :: resolve 494ms :: artifacts dl 8ms
	:: modules in use:
	com.squareup.moshi#moshi;1.5.0 from central in [default]
	com.squareup.okhttp3#logging-interceptor;3.11.0 from central in [default]
	com.squareup.okhttp3#okhttp;3.11.0 from central in [default]
	com.squareup.okio#okio;1.14.0 from central in [default]
	com.squareup.retrofit2#converter-moshi;2.4.0 from central in [default]
	com.squareup.retrofit2#retrofit;2.4.0 from central in [default]
	graphframes#graphframes;0.7.0-spark2.4-s_2.11 from spark-packages in [default]
	org.influxdb#influxdb-java;2.14 from central in [default]
	org.msgpack#msgpack-core;0.8.16 from central in [default]
	org.slf4j#slf4j-api;1.7.16 from central in [default]
	:: evicted modules:
	com.squareup.okhttp3#okhttp;3.10.0 by [com.squareup.okhttp3#okhttp;3.11.0] in [default]
	com.squareup.okio#okio;1.13.0 by [com.squareup.okio#okio;1.14.0] in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   12  |   0   |   0   |   2   ||   10  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-e9ca6de6-60ae-4243-a15c-0232349dccb3
	confs: [default]
	0 artifacts copied, 10 already retrieved (0kB/9ms)
20/04/17 10:25:55 INFO spark.SparkContext: Running Spark version 2.4.3
20/04/17 10:25:55 WARN spark.SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
20/04/17 10:25:55 INFO spark.SparkContext: Submitted application: GraphX
20/04/17 10:25:55 INFO spark.SecurityManager: Changing view acls to: chris.arnault
20/04/17 10:25:55 INFO spark.SecurityManager: Changing modify acls to: chris.arnault
20/04/17 10:25:55 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/17 10:25:55 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/17 10:25:55 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(chris.arnault); groups with view permissions: Set(); users  with modify permissions: Set(chris.arnault); groups with modify permissions: Set()
20/04/17 10:25:55 INFO util.Utils: Successfully started service 'sparkDriver' on port 40055.
20/04/17 10:25:55 INFO spark.SparkEnv: Registering MapOutputTracker
20/04/17 10:25:55 INFO spark.SparkEnv: Registering BlockManagerMaster
20/04/17 10:25:55 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/04/17 10:25:55 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/04/17 10:25:55 INFO storage.DiskBlockManager: Created local directory at /data2/spark_local/blockmgr-88288bf6-dffa-4c19-a0d1-3c78537e3856
20/04/17 10:25:55 INFO memory.MemoryStore: MemoryStore started with capacity 15.3 GB
20/04/17 10:25:55 INFO spark.SparkEnv: Registering OutputCommitCoordinator
20/04/17 10:25:55 INFO util.log: Logging initialized @3611ms
20/04/17 10:25:55 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
20/04/17 10:25:55 INFO server.Server: Started @3687ms
20/04/17 10:25:55 WARN util.Utils: Service 'SparkUI' could not bind on port 20100. Attempting port 20101.
20/04/17 10:25:55 WARN util.Utils: Service 'SparkUI' could not bind on port 20101. Attempting port 20102.
20/04/17 10:25:55 INFO server.AbstractConnector: Started ServerConnector@771fceb8{HTTP/1.1,[http/1.1]}{0.0.0.0:20102}
20/04/17 10:25:55 INFO util.Utils: Successfully started service 'SparkUI' on port 20102.
20/04/17 10:25:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a6a535f{/jobs,null,AVAILABLE,@Spark}
20/04/17 10:25:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@716d4688{/jobs/json,null,AVAILABLE,@Spark}
20/04/17 10:25:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5465bbe2{/jobs/job,null,AVAILABLE,@Spark}
20/04/17 10:25:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4e64e6b6{/jobs/job/json,null,AVAILABLE,@Spark}
20/04/17 10:25:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@55a6b0f4{/stages,null,AVAILABLE,@Spark}
20/04/17 10:25:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c65bb09{/stages/json,null,AVAILABLE,@Spark}
20/04/17 10:25:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@646e473{/stages/stage,null,AVAILABLE,@Spark}
20/04/17 10:25:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@e83805d{/stages/stage/json,null,AVAILABLE,@Spark}
20/04/17 10:25:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3fd79882{/stages/pool,null,AVAILABLE,@Spark}
20/04/17 10:25:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@272bc018{/stages/pool/json,null,AVAILABLE,@Spark}
20/04/17 10:25:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@561fcef2{/storage,null,AVAILABLE,@Spark}
20/04/17 10:25:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@448c3d04{/storage/json,null,AVAILABLE,@Spark}
20/04/17 10:25:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@471cbe9f{/storage/rdd,null,AVAILABLE,@Spark}
20/04/17 10:25:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ece9d64{/storage/rdd/json,null,AVAILABLE,@Spark}
20/04/17 10:25:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@168ebc11{/environment,null,AVAILABLE,@Spark}
20/04/17 10:25:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@334fd7d9{/environment/json,null,AVAILABLE,@Spark}
20/04/17 10:25:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@11057f69{/executors,null,AVAILABLE,@Spark}
20/04/17 10:25:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5beaf590{/executors/json,null,AVAILABLE,@Spark}
20/04/17 10:25:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3867a4c8{/executors/threadDump,null,AVAILABLE,@Spark}
20/04/17 10:25:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@75db5d02{/executors/threadDump/json,null,AVAILABLE,@Spark}
20/04/17 10:25:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4f9f8ae7{/static,null,AVAILABLE,@Spark}
20/04/17 10:25:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@12ed62af{/,null,AVAILABLE,@Spark}
20/04/17 10:25:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e384abc{/api,null,AVAILABLE,@Spark}
20/04/17 10:25:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@284fbf41{/jobs/job/kill,null,AVAILABLE,@Spark}
20/04/17 10:25:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f954e88{/stages/stage/kill,null,AVAILABLE,@Spark}
20/04/17 10:25:55 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://vm-75222.lal.in2p3.fr:20102
20/04/17 10:25:55 INFO spark.SparkContext: Added JAR file:///opt/spark-2/sparkMeasure/target/scala-2.11/spark-measure_2.11-0.16-SNAPSHOT.jar at spark://vm-75222.lal.in2p3.fr:40055/jars/spark-measure_2.11-0.16-SNAPSHOT.jar with timestamp 1587111955892
20/04/17 10:25:55 INFO spark.SparkContext: Added JAR file:///home/chris.arnault/.ivy2/jars/org.influxdb_influxdb-java-2.14.jar at spark://vm-75222.lal.in2p3.fr:40055/jars/org.influxdb_influxdb-java-2.14.jar with timestamp 1587111955893
20/04/17 10:25:55 INFO spark.SparkContext: Added JAR file:///home/chris.arnault/.ivy2/jars/graphframes_graphframes-0.7.0-spark2.4-s_2.11.jar at spark://vm-75222.lal.in2p3.fr:40055/jars/graphframes_graphframes-0.7.0-spark2.4-s_2.11.jar with timestamp 1587111955893
20/04/17 10:25:55 INFO spark.SparkContext: Added JAR file:///home/chris.arnault/.ivy2/jars/com.squareup.retrofit2_retrofit-2.4.0.jar at spark://vm-75222.lal.in2p3.fr:40055/jars/com.squareup.retrofit2_retrofit-2.4.0.jar with timestamp 1587111955893
20/04/17 10:25:55 INFO spark.SparkContext: Added JAR file:///home/chris.arnault/.ivy2/jars/com.squareup.retrofit2_converter-moshi-2.4.0.jar at spark://vm-75222.lal.in2p3.fr:40055/jars/com.squareup.retrofit2_converter-moshi-2.4.0.jar with timestamp 1587111955893
20/04/17 10:25:55 INFO spark.SparkContext: Added JAR file:///home/chris.arnault/.ivy2/jars/org.msgpack_msgpack-core-0.8.16.jar at spark://vm-75222.lal.in2p3.fr:40055/jars/org.msgpack_msgpack-core-0.8.16.jar with timestamp 1587111955893
20/04/17 10:25:55 INFO spark.SparkContext: Added JAR file:///home/chris.arnault/.ivy2/jars/com.squareup.okhttp3_okhttp-3.11.0.jar at spark://vm-75222.lal.in2p3.fr:40055/jars/com.squareup.okhttp3_okhttp-3.11.0.jar with timestamp 1587111955894
20/04/17 10:25:55 INFO spark.SparkContext: Added JAR file:///home/chris.arnault/.ivy2/jars/com.squareup.okhttp3_logging-interceptor-3.11.0.jar at spark://vm-75222.lal.in2p3.fr:40055/jars/com.squareup.okhttp3_logging-interceptor-3.11.0.jar with timestamp 1587111955894
20/04/17 10:25:55 INFO spark.SparkContext: Added JAR file:///home/chris.arnault/.ivy2/jars/com.squareup.moshi_moshi-1.5.0.jar at spark://vm-75222.lal.in2p3.fr:40055/jars/com.squareup.moshi_moshi-1.5.0.jar with timestamp 1587111955894
20/04/17 10:25:55 INFO spark.SparkContext: Added JAR file:///home/chris.arnault/.ivy2/jars/com.squareup.okio_okio-1.14.0.jar at spark://vm-75222.lal.in2p3.fr:40055/jars/com.squareup.okio_okio-1.14.0.jar with timestamp 1587111955894
20/04/17 10:25:55 INFO spark.SparkContext: Added JAR file:///home/chris.arnault/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://vm-75222.lal.in2p3.fr:40055/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1587111955894
20/04/17 10:25:55 INFO spark.SparkContext: Added file file:///home/chris.arnault/.ivy2/jars/org.influxdb_influxdb-java-2.14.jar at spark://vm-75222.lal.in2p3.fr:40055/files/org.influxdb_influxdb-java-2.14.jar with timestamp 1587111955921
20/04/17 10:25:55 INFO util.Utils: Copying /home/chris.arnault/.ivy2/jars/org.influxdb_influxdb-java-2.14.jar to /data2/spark_local/spark-8372fde5-8386-4968-8d90-8802bbc765ce/userFiles-f68b69d8-14fd-4fe5-9972-e2813c6e3d11/org.influxdb_influxdb-java-2.14.jar
20/04/17 10:25:55 INFO spark.SparkContext: Added file file:///home/chris.arnault/.ivy2/jars/graphframes_graphframes-0.7.0-spark2.4-s_2.11.jar at spark://vm-75222.lal.in2p3.fr:40055/files/graphframes_graphframes-0.7.0-spark2.4-s_2.11.jar with timestamp 1587111955931
20/04/17 10:25:55 INFO util.Utils: Copying /home/chris.arnault/.ivy2/jars/graphframes_graphframes-0.7.0-spark2.4-s_2.11.jar to /data2/spark_local/spark-8372fde5-8386-4968-8d90-8802bbc765ce/userFiles-f68b69d8-14fd-4fe5-9972-e2813c6e3d11/graphframes_graphframes-0.7.0-spark2.4-s_2.11.jar
20/04/17 10:25:55 INFO spark.SparkContext: Added file file:///home/chris.arnault/.ivy2/jars/com.squareup.retrofit2_retrofit-2.4.0.jar at spark://vm-75222.lal.in2p3.fr:40055/files/com.squareup.retrofit2_retrofit-2.4.0.jar with timestamp 1587111955935
20/04/17 10:25:55 INFO util.Utils: Copying /home/chris.arnault/.ivy2/jars/com.squareup.retrofit2_retrofit-2.4.0.jar to /data2/spark_local/spark-8372fde5-8386-4968-8d90-8802bbc765ce/userFiles-f68b69d8-14fd-4fe5-9972-e2813c6e3d11/com.squareup.retrofit2_retrofit-2.4.0.jar
20/04/17 10:25:55 INFO spark.SparkContext: Added file file:///home/chris.arnault/.ivy2/jars/com.squareup.retrofit2_converter-moshi-2.4.0.jar at spark://vm-75222.lal.in2p3.fr:40055/files/com.squareup.retrofit2_converter-moshi-2.4.0.jar with timestamp 1587111955939
20/04/17 10:25:55 INFO util.Utils: Copying /home/chris.arnault/.ivy2/jars/com.squareup.retrofit2_converter-moshi-2.4.0.jar to /data2/spark_local/spark-8372fde5-8386-4968-8d90-8802bbc765ce/userFiles-f68b69d8-14fd-4fe5-9972-e2813c6e3d11/com.squareup.retrofit2_converter-moshi-2.4.0.jar
20/04/17 10:25:55 INFO spark.SparkContext: Added file file:///home/chris.arnault/.ivy2/jars/org.msgpack_msgpack-core-0.8.16.jar at spark://vm-75222.lal.in2p3.fr:40055/files/org.msgpack_msgpack-core-0.8.16.jar with timestamp 1587111955942
20/04/17 10:25:55 INFO util.Utils: Copying /home/chris.arnault/.ivy2/jars/org.msgpack_msgpack-core-0.8.16.jar to /data2/spark_local/spark-8372fde5-8386-4968-8d90-8802bbc765ce/userFiles-f68b69d8-14fd-4fe5-9972-e2813c6e3d11/org.msgpack_msgpack-core-0.8.16.jar
20/04/17 10:25:55 INFO spark.SparkContext: Added file file:///home/chris.arnault/.ivy2/jars/com.squareup.okhttp3_okhttp-3.11.0.jar at spark://vm-75222.lal.in2p3.fr:40055/files/com.squareup.okhttp3_okhttp-3.11.0.jar with timestamp 1587111955946
20/04/17 10:25:55 INFO util.Utils: Copying /home/chris.arnault/.ivy2/jars/com.squareup.okhttp3_okhttp-3.11.0.jar to /data2/spark_local/spark-8372fde5-8386-4968-8d90-8802bbc765ce/userFiles-f68b69d8-14fd-4fe5-9972-e2813c6e3d11/com.squareup.okhttp3_okhttp-3.11.0.jar
20/04/17 10:25:55 INFO spark.SparkContext: Added file file:///home/chris.arnault/.ivy2/jars/com.squareup.okhttp3_logging-interceptor-3.11.0.jar at spark://vm-75222.lal.in2p3.fr:40055/files/com.squareup.okhttp3_logging-interceptor-3.11.0.jar with timestamp 1587111955949
20/04/17 10:25:55 INFO util.Utils: Copying /home/chris.arnault/.ivy2/jars/com.squareup.okhttp3_logging-interceptor-3.11.0.jar to /data2/spark_local/spark-8372fde5-8386-4968-8d90-8802bbc765ce/userFiles-f68b69d8-14fd-4fe5-9972-e2813c6e3d11/com.squareup.okhttp3_logging-interceptor-3.11.0.jar
20/04/17 10:25:55 INFO spark.SparkContext: Added file file:///home/chris.arnault/.ivy2/jars/com.squareup.moshi_moshi-1.5.0.jar at spark://vm-75222.lal.in2p3.fr:40055/files/com.squareup.moshi_moshi-1.5.0.jar with timestamp 1587111955952
20/04/17 10:25:55 INFO util.Utils: Copying /home/chris.arnault/.ivy2/jars/com.squareup.moshi_moshi-1.5.0.jar to /data2/spark_local/spark-8372fde5-8386-4968-8d90-8802bbc765ce/userFiles-f68b69d8-14fd-4fe5-9972-e2813c6e3d11/com.squareup.moshi_moshi-1.5.0.jar
20/04/17 10:25:55 INFO spark.SparkContext: Added file file:///home/chris.arnault/.ivy2/jars/com.squareup.okio_okio-1.14.0.jar at spark://vm-75222.lal.in2p3.fr:40055/files/com.squareup.okio_okio-1.14.0.jar with timestamp 1587111955956
20/04/17 10:25:55 INFO util.Utils: Copying /home/chris.arnault/.ivy2/jars/com.squareup.okio_okio-1.14.0.jar to /data2/spark_local/spark-8372fde5-8386-4968-8d90-8802bbc765ce/userFiles-f68b69d8-14fd-4fe5-9972-e2813c6e3d11/com.squareup.okio_okio-1.14.0.jar
20/04/17 10:25:55 INFO spark.SparkContext: Added file file:///home/chris.arnault/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://vm-75222.lal.in2p3.fr:40055/files/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1587111955960
20/04/17 10:25:55 INFO util.Utils: Copying /home/chris.arnault/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /data2/spark_local/spark-8372fde5-8386-4968-8d90-8802bbc765ce/userFiles-f68b69d8-14fd-4fe5-9972-e2813c6e3d11/org.slf4j_slf4j-api-1.7.16.jar
20/04/17 10:25:56 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://134.158.75.222:7077...
20/04/17 10:25:56 INFO client.TransportClientFactory: Successfully created connection to /134.158.75.222:7077 after 33 ms (0 ms spent in bootstraps)
20/04/17 10:25:56 INFO cluster.StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20200417102556-0256
20/04/17 10:25:56 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20200417102556-0256/0 on worker-20200326093227-134.158.75.171-46325 (134.158.75.171:46325) with 17 core(s)
20/04/17 10:25:56 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20200417102556-0256/0 on hostPort 134.158.75.171:46325 with 17 core(s), 29.0 GB RAM
20/04/17 10:25:56 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20200417102556-0256/1 on worker-20200326093234-134.158.75.183-34688 (134.158.75.183:34688) with 17 core(s)
20/04/17 10:25:56 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20200417102556-0256/1 on hostPort 134.158.75.183:34688 with 17 core(s), 29.0 GB RAM
20/04/17 10:25:56 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20200417102556-0256/2 on worker-20200326093240-134.158.74.164-43399 (134.158.74.164:43399) with 17 core(s)
20/04/17 10:25:56 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20200417102556-0256/2 on hostPort 134.158.74.164:43399 with 17 core(s), 29.0 GB RAM
20/04/17 10:25:56 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20200417102556-0256/3 on worker-20200326093235-134.158.75.133-37914 (134.158.75.133:37914) with 17 core(s)
20/04/17 10:25:56 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20200417102556-0256/3 on hostPort 134.158.75.133:37914 with 17 core(s), 29.0 GB RAM
20/04/17 10:25:56 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37212.
20/04/17 10:25:56 INFO netty.NettyBlockTransferService: Server created on vm-75222.lal.in2p3.fr:37212
20/04/17 10:25:56 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/17 10:25:56 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20200417102556-0256/1 is now RUNNING
20/04/17 10:25:56 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20200417102556-0256/0 is now RUNNING
20/04/17 10:25:56 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20200417102556-0256/3 is now RUNNING
20/04/17 10:25:56 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20200417102556-0256/2 is now RUNNING
20/04/17 10:25:56 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, vm-75222.lal.in2p3.fr, 37212, None)
20/04/17 10:25:56 INFO storage.BlockManagerMasterEndpoint: Registering block manager vm-75222.lal.in2p3.fr:37212 with 15.3 GB RAM, BlockManagerId(driver, vm-75222.lal.in2p3.fr, 37212, None)
20/04/17 10:25:56 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, vm-75222.lal.in2p3.fr, 37212, None)
20/04/17 10:25:56 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, vm-75222.lal.in2p3.fr, 37212, None)
20/04/17 10:25:56 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@85d3c3{/metrics/json,null,AVAILABLE,@Spark}
20/04/17 10:25:57 INFO scheduler.EventLoggingListener: Logging events to hdfs://134.158.75.222/spark-history/app-20200417102556-0256
20/04/17 10:25:57 WARN sparkmeasure.InfluxDBSinkExtended: Custom monitoring listener with InfluxDB sink initializing. Now attempting to connect to InfluxDB
20/04/17 10:25:57 INFO sparkmeasure.InfluxDBSinkExtended: Found URL for InfluxDB: http://supervision.lal.in2p3.fr:8086
20/04/17 10:25:57 WARN sparkmeasure.InfluxDBSinkExtended: Credentials for InfluxDB connection not found, using empty username and password, InfluxDB must be running with auth-enabled=false
20/04/17 10:25:57 INFO sparkmeasure.InfluxDBSinkExtended: InfluxDB name: sparkmeasure
20/04/17 10:25:57 INFO sparkmeasure.InfluxDBSinkExtended: using InfluxDB database sparkmeasure
20/04/17 10:25:57 INFO sparkmeasure.InfluxDBSinkExtended: Log also stagemetrics: true
20/04/17 10:25:57 INFO spark.SparkContext: Registered listener ch.cern.sparkmeasure.InfluxDBSinkExtended
20/04/17 10:25:57 INFO cluster.StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
20/04/17 10:25:57 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:///user/spark/warehouse').
20/04/17 10:25:57 INFO internal.SharedState: Warehouse path is 'file:///user/spark/warehouse'.
20/04/17 10:25:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7705a50d{/SQL,null,AVAILABLE,@Spark}
20/04/17 10:25:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6f3321a8{/SQL/json,null,AVAILABLE,@Spark}
20/04/17 10:25:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7b63e033{/SQL/execution,null,AVAILABLE,@Spark}
20/04/17 10:25:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c7bc558{/SQL/execution/json,null,AVAILABLE,@Spark}
20/04/17 10:25:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18b558a9{/static/sql,null,AVAILABLE,@Spark}
20/04/17 10:25:58 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (134.158.74.164:53266) with ID 2
20/04/17 10:25:58 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (134.158.75.133:54822) with ID 3
20/04/17 10:25:58 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (134.158.75.171:39146) with ID 0
20/04/17 10:25:58 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (134.158.75.183:56666) with ID 1
20/04/17 10:25:58 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
graphs=
batches_edges = 10
batches_vertices = 10
degree_max = 1000
file_format = parquet
g = 100
graphs = /user/chris.arnault/graphs/test_N10000_BN10_BE10_D1000_G10000
graphs_base = /user/chris.arnault/graphs
grid = 10000
name = test
partitions = 300
read_vertices = False
set = <bound method Conf.set of <__main__.Conf object at 0x7fb06fc48be0>>
vertices = 10000
mkdir: `/user/chris.arnault/graphs/test_N10000_BN10_BE10_D1000_G10000': File exists
Deleted /user/chris.arnault/graphs/test_N10000_BN10_BE10_D1000_G10000/vertices
batch_create>  /user/chris.arnault/graphs/test_N10000_BN10_BE10_D1000_G10000 vertices total_rows= 10000 batches= 10
batch>  0  range  0 1000
-------------------------------- create dataframe | 0h0m1.791s
-------------------------------- Write block | 0h0m6.075s
file_size=0.0966796875 increment=0.0966796875
batch>  1  range  1000 2000
-------------------------------- create dataframe | 0h0m2.129s
-------------------------------- Write block | 0h0m1.607s
file_size=0.193359375 increment=0.0966796875
batch>  2  range  2000 3000
-------------------------------- create dataframe | 0h0m2.103s
-------------------------------- Write block | 0h0m1.834s
file_size=0.2900390625 increment=0.0966796875
batch>  3  range  3000 4000
-------------------------------- create dataframe | 0h0m2.125s
-------------------------------- Write block | 0h0m2.169s
file_size=0.38662109375 increment=0.09658203124999998
batch>  4  range  4000 5000
-------------------------------- create dataframe | 0h0m2.064s
-------------------------------- Write block | 0h0m2.259s
file_size=0.48330078125 increment=0.0966796875
batch>  5  range  5000 6000
-------------------------------- create dataframe | 0h0m2.100s
-------------------------------- Write block | 0h0m5.557s
file_size=0.57998046875 increment=0.0966796875
batch>  6  range  6000 7000
-------------------------------- create dataframe | 0h0m2.149s
-------------------------------- Write block | 0h0m2.347s
file_size=0.67666015625 increment=0.0966796875
batch>  7  range  7000 8000
-------------------------------- create dataframe | 0h0m2.075s
-------------------------------- Write block | 0h0m2.076s
file_size=0.77333984375 increment=0.0966796875
batch>  8  range  8000 9000
-------------------------------- create dataframe | 0h0m2.125s
-------------------------------- Write block | 0h0m1.844s
file_size=0.869921875 increment=0.09658203124999998
batch>  9  range  9000 10000
-------------------------------- create dataframe | 0h0m2.097s
-------------------------------- Write block | 0h0m3.150s
file_size=0.96669921875 increment=0.09677734375000002
-------------------------------- Read full file | 0h0m2.722s
-------------------------------- creating vertices | 0h0m54.576s
original partitions # = 68
effective partitions # = 300
Deleted /user/chris.arnault/graphs/test_N10000_BN10_BE10_D1000_G10000/edges_temp
batch_create>  /user/chris.arnault/graphs/test_N10000_BN10_BE10_D1000_G10000 edges_temp total_rows= 10000 batches= 10
batch>  0  range  0 1000
-------------------------------- create dataframe and join | 0h0m16.296s
-------------------------------- Write block | 0h0m4.994s
file_size=0.05146484375 increment=0.05146484375
batch>  1  range  1000 2000
-------------------------------- create dataframe and join | 0h0m18.144s
-------------------------------- Write block | 0h0m3.388s
file_size=0.10537109375 increment=0.05390625
batch>  2  range  2000 3000
-------------------------------- create dataframe and join | 0h0m17.927s
-------------------------------- Write block | 0h0m2.963s
file_size=0.15693359375 increment=0.05156249999999998
batch>  3  range  3000 4000
-------------------------------- create dataframe and join | 0h0m18.287s
-------------------------------- Write block | 0h0m2.553s
file_size=0.2107421875 increment=0.05380859375000002
batch>  4  range  4000 5000
-------------------------------- create dataframe and join | 0h0m17.920s
-------------------------------- Write block | 0h0m3.736s
file_size=0.26435546875 increment=0.05361328124999998
batch>  5  range  5000 6000
-------------------------------- create dataframe and join | 0h0m17.649s
-------------------------------- Write block | 0h0m2.363s
file_size=0.31884765625 increment=0.05449218750000001
batch>  6  range  6000 7000
-------------------------------- create dataframe and join | 0h0m17.660s
-------------------------------- Write block | 0h0m2.300s
file_size=0.37138671875 increment=0.05253906250000001
batch>  7  range  7000 8000
-------------------------------- create dataframe and join | 0h0m17.842s
-------------------------------- Write block | 0h0m2.791s
file_size=0.4220703125 increment=0.05068359374999998
batch>  8  range  8000 9000
-------------------------------- create dataframe and join | 0h0m18.319s
-------------------------------- Write block | 0h0m3.584s
file_size=0.47314453125 increment=0.05107421875000001
batch>  9  range  9000 10000
-------------------------------- create dataframe and join | 0h0m17.973s
-------------------------------- Write block | 0h0m3.281s
file_size=0.528515625 increment=0.055371093750000044
-------------------------------- Read full file | 0h0m2.487s
-------------------------------- Create a GraphFrame | 0h3m35.159s
count: vertices= 10000 edges= 1452
-------------------------------- count GraphFrame | 0h0m1.374s
+----+--------------------+--------------------+----+
|  id|                   x|                   y|cell|
+----+--------------------+--------------------+----+
|5487| 0.13918398231431006| 0.18089649402278873|1813|
|5241|  0.6172281082403885|  0.3024568299811905|3061|
|2738|  0.3046666299072227| 0.07368962430047654| 730|
|8729| 0.19687277922233937|0.008202117452342117|  19|
|8497| 0.12114407404237826|  0.3104659133717088|3112|
|7251|  0.8812091395945613| 0.20665977752802667|2088|
|5227|  0.8266000832708594|   0.161967254293236|1682|
|2730| 0.04595812508581554|  0.5391712703229391|5304|
|6499| 0.08704835107531672|  0.8183259508152881|8108|
|5503|  0.9376052947353983|  0.9882066718206493|9893|
|5233|  0.5366530340813295| 0.08125749449520159| 853|
|5735| 0.19609632068344562| 0.13120767087241125|1319|
|6497|  0.8958166582042802|   0.950778393776184|9589|
|7239| 0.16975323635184258|  0.7082167875222108|7016|
|8752|0.026054141916090567| 0.20619162808181157|2002|
|5225| 0.13690104013535376|  0.5805214988806069|5813|
|5243| 0.24796580371406596|   0.949954106158971|9424|
|1751|  0.5134672043095827|  0.5069637345377535|5051|
|8733|  0.8685901625125901|  0.5051156975083267|5086|
|5488|  0.5362534821644668| 0.07672542316191122| 753|
+----+--------------------+--------------------+----+
only showing top 20 rows

+------+----+----+
|   eid| src| dst|
+------+----+----+
|471099|2956|9486|
|471874|2957|4178|
|472225|2958|8321|
|472317|2958|1896|
|472773|2960|6298|
|473078|2960|7560|
|473636|2962|8994|
|474892|2965|3733|
|477152|2969|5894|
| 22112|7048|1309|
| 23564|7050| 471|
| 23809|7050| 471|
| 25919|7053|1704|
| 26098|7054|2332|
| 26716|7055|5435|
| 27928|7057| 649|
|429531|1872| 827|
|429582|1872| 827|
|431261|1875| 770|
|431643|1875| 770|
+------+----+----+
only showing top 20 rows

